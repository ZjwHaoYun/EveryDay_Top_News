2026-01-28 12:32:25,269 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000023781AD0A70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:32:25,279 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共7个，每批并发5个
2026-01-28 12:32:25,279 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 12:32:25,282 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000023781AD2450>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:32:25,295 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共7个，每批并发5个
2026-01-28 12:32:25,295 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 12:32:32,068 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 12:32:32,068 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 12:32:32,068 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 12:32:32,068 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 12:32:32,068 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_baijingchuhai_data执行失败：RetryError[<Future at 0x23781aeade0 state=finished raised TypeError>]
NoneType: None
2026-01-28 12:32:32,068 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.99秒执行下一批
2026-01-28 12:32:34,766 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 12:32:34,766 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 12:32:34,766 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 12:32:34,766 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 12:32:34,766 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_baijingchuhai_data执行失败：RetryError[<Future at 0x23781ad39b0 state=finished raised TypeError>]
NoneType: None
2026-01-28 12:32:34,766 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.58秒执行下一批
2026-01-28 12:32:35,061 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共2个
2026-01-28 12:32:37,337 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共2个
2026-01-28 12:32:37,903 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 12:32:37,903 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 12:32:37,903 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合6个站点的热点数据
2026-01-28 12:32:41,191 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 12:32:41,191 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 12:32:41,191 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合6个站点的热点数据
2026-01-28 12:33:46,759 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000129D66E8BF0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:33:46,769 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共7个，每批并发5个
2026-01-28 12:33:46,769 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 12:33:50,367 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 12:33:50,367 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 12:33:50,367 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 12:33:50,367 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 12:33:50,367 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_baijingchuhai_data执行失败：RetryError[<Future at 0x129d64b1f70 state=finished raised TypeError>]
NoneType: None
2026-01-28 12:33:50,367 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.91秒执行下一批
2026-01-28 12:33:53,295 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共2个
2026-01-28 12:33:56,836 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 12:33:56,836 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 12:33:56,836 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合6个站点的热点数据
2026-01-28 12:34:22,847 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001B5BDBF8CE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:34:22,859 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共7个，每批并发5个
2026-01-28 12:34:22,859 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 12:34:25,810 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 12:34:25,810 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 12:34:25,810 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 12:34:25,810 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 12:34:25,810 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 12:34:25,811 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.73秒执行下一批
2026-01-28 12:34:28,534 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共2个
2026-01-28 12:34:31,898 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 12:34:31,898 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 12:34:31,898 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合7个站点的热点数据
2026-01-28 12:37:24,514 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000025A62E68C80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:37:24,537 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 12:37:24,541 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000025A62E6AE40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 12:37:24,561 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:36:42,970 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000140FE348CB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:36:42,983 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共8个，每批并发5个
2026-01-28 13:36:42,983 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 13:36:45,901 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.43秒执行下一批
2026-01-28 13:36:48,328 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共3个
2026-01-28 13:36:53,162 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 13:36:53,162 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 13:36:53,162 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 13:36:53,162 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-28 13:37:24,561 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000140FE34B4A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:37:24,580 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:38:15,656 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001DF43F88AA0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:38:15,664 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共8个，每批并发5个
2026-01-28 13:38:15,665 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 13:38:18,019 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.08秒执行下一批
2026-01-28 13:38:20,111 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共3个
2026-01-28 13:38:23,165 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 13:38:23,165 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 13:38:23,165 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 13:38:23,165 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-28 13:39:28,570 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000027837498890>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:39:28,582 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共8个，每批并发5个
2026-01-28 13:39:28,583 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 13:39:31,020 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 13:39:31,021 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 13:39:31,021 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 13:39:31,021 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 13:39:31,021 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 13:39:31,022 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.17秒执行下一批
2026-01-28 13:39:33,199 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共3个
2026-01-28 13:39:36,881 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 13:39:36,881 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 13:39:36,881 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 13:39:36,881 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-28 13:42:24,574 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000278374BF0E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:42:24,596 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:44:29,565 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001801A6E87D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:44:29,585 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:47:24,564 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000021F0A038B60>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:47:24,582 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:49:29,566 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000021F0A03B8F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:49:29,592 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 13:52:24,566 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000011072A88A40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 13:52:24,584 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:10:35,741 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001BB1EB38C20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:10:35,752 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-28 14:10:35,752 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 14:10:38,118 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.48秒执行下一批
2026-01-28 14:10:40,614 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-28 14:10:45,485 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 14:10:45,485 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 14:10:45,485 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 14:10:45,485 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-28 14:10:45,485 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-28 14:12:06,683 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208B90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:12:06,694 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-28 14:12:06,694 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 14:12:09,831 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.79秒执行下一批
2026-01-28 14:12:12,624 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-28 14:12:16,660 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 14:12:16,660 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 14:12:16,660 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 14:12:16,660 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-28 14:12:16,660 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-28 14:12:24,575 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208740>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:12:24,595 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:13:12,180 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720AC60>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:13:12,199 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:17:07,601 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720A780>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:17:07,623 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:17:24,589 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209A90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:17:24,611 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:22:06,725 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720AEA0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:22:06,747 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:22:24,619 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209EB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:22:24,639 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:27:07,616 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720ACC0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:27:07,632 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:27:24,619 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B170>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:27:24,641 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:29:54,652 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720A9C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:29:54,678 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:32:07,626 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209730>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:32:07,645 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:32:24,625 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209610>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:32:24,645 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:34:55,635 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720A4B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:34:55,653 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:37:07,621 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209130>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:37:07,636 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:37:24,629 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208EC0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:37:24,645 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:39:55,624 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208AD0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:39:55,650 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:41:00,122 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208D40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:41:00,132 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:42:07,629 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208680>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:42:07,645 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:42:24,629 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208D10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:42:24,640 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:44:55,632 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208740>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:44:55,655 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:45:21,905 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209EE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:45:21,918 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:45:31,141 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720A9C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:45:31,159 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:45:50,509 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209550>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:45:50,522 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:47:24,645 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720A750>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:47:24,665 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:50:31,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208D10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:50:31,653 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:50:50,646 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209EE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:50:50,657 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:52:24,643 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209A60>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:52:24,656 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:53:53,163 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208FE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:53:53,187 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:55:31,648 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208590>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:55:31,671 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:55:50,641 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208FE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:55:50,666 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:57:48,823 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209820>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:57:48,847 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:57:48,849 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720BA40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:57:48,861 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:58:32,039 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720AC90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:58:32,059 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:58:32,061 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209820>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:58:32,081 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 14:58:53,635 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B0B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 14:58:53,650 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:00:31,647 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B740>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:00:31,668 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:00:50,646 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209D60>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:00:50,657 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:02:49,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720AF00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:02:49,654 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:02:49,659 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B770>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:02:49,677 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:03:32,646 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272095E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:03:32,670 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:03:32,673 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209C70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:03:32,691 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:03:53,638 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208650>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:03:53,660 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:05:31,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B6B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:05:31,656 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:05:50,642 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208650>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:05:50,663 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:07:49,639 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A92720B470>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:07:49,658 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:07:49,661 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272328A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:07:49,680 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:08:32,633 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927208740>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:08:32,649 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:08:32,664 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230A40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:08:32,681 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:08:53,633 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209550>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:08:53,646 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:11:21,863 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209B80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:11:21,883 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:11:25,362 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233E90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:11:25,379 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:12:25,316 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233350>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:12:25,332 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:12:49,648 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927209550>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:12:49,667 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:13:32,638 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272311F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:13:32,661 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:13:53,644 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233620>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:13:53,657 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:16:22,633 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230BF0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:16:22,649 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:17:25,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231EB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:17:25,650 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:17:49,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231D60>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:17:49,657 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:18:32,644 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233590>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:18:32,660 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:19:24,649 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233B90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:19:24,667 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:21:22,659 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272306E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:21:22,675 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:22:25,661 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230980>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:22:25,680 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:23:24,665 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233CB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:23:24,694 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:24:24,676 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231760>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:24:24,694 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:24:24,705 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233CB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:24:24,715 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:26:22,679 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233380>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:26:22,691 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:27:25,673 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231B20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:27:25,695 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:28:24,679 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230C50>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:28:24,699 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:24,679 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231D30>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:24,699 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:24,705 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927232150>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:24,724 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:36,326 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272305F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:36,347 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:36,352 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272309E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:36,363 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:39,869 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272303E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:39,888 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:39,895 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272339B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:39,914 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:53,646 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272320C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:53,661 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:53,661 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233DA0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:53,688 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:53,691 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233A70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:53,703 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:29:53,713 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231D30>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:29:53,732 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:33:24,696 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231730>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:33:24,715 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:34:24,701 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230290>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:34:24,721 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:34:24,726 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230860>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:34:24,742 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:38:24,700 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231340>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:38:24,726 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:39:24,694 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233A70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:39:24,723 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:39:24,725 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230200>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:39:24,741 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:24,700 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233B00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:24,709 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:36,019 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272303E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:36,039 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:36,052 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233620>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:36,072 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:36,134 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230CB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:36,156 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:36,168 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272307D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:36,195 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:43,207 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233BF0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:43,229 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:43:43,235 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272307A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:43:43,254 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:57:10,327 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272321B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:57:10,347 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:57:10,350 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272321E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:57:10,367 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:57:10,372 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272307D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:57:10,391 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 15:57:10,394 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272300E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 15:57:10,413 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:00:12,591 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230D10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:00:12,608 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:00:12,608 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233FE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:00:12,633 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:00:12,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927231160>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:00:12,660 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:00:12,662 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233B00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:00:12,682 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:03:45,045 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927230860>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:03:45,064 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:03:45,064 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272316D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:03:45,087 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:03:45,091 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A927233F20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:03:45,110 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:03:45,113 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272311C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:03:45,124 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:04:31,492 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A9272326C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:04:31,512 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-28 16:04:31,512 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 16:04:34,425 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.47秒执行下一批
2026-01-28 16:04:36,907 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-28 16:04:42,458 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 16:04:42,458 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 16:04:42,458 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 16:04:42,458 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-28 16:04:42,458 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-28 16:13:21,738 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000221A5F58B00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:13:21,746 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-28 16:13:21,746 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 16:13:43,610 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.49秒执行下一批
2026-01-28 16:13:46,094 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-28 16:13:50,644 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 16:13:50,644 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 16:13:50,644 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 16:13:50,644 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-28 16:13:50,644 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-28 16:19:00,998 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000013B53048FE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:19:01,012 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-28 16:19:01,012 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-28 16:19:04,102 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.55秒执行下一批
2026-01-28 16:19:06,660 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-28 16:19:10,175 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-28 16:19:10,175 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-28 16:19:10,177 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-28 16:19:10,177 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-28 16:19:10,177 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-28 16:19:10,192 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000013B5306DBE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:19:10,207 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:26:44,853 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309C10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:26:44,876 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:26:44,879 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930BDA0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:26:44,902 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:26:49,869 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309A90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:26:49,889 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:26:49,894 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309970>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:26:49,915 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:11,363 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A090>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:11,383 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:11,388 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B710>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:11,408 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:13,198 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A2D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:13,216 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:13,223 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B6B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:13,243 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:19,665 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930ADE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:19,684 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:27:19,690 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930BA10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:27:19,708 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:47:36,610 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B080>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:47:36,626 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:47:36,637 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B890>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:47:36,657 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:47:36,660 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A8D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:47:36,678 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:47:36,684 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B380>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:47:36,702 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:52:05,109 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A870>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:52:05,128 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:52:05,131 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309CD0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:52:05,144 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:52:05,149 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B890>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:52:05,167 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:52:05,172 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B8C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:52:05,191 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:29,623 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930AAB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:29,646 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:29,649 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309C10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:29,665 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:29,672 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930AAB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:29,691 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:29,694 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC93097C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:29,714 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:33,223 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309B80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:33,243 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:53:33,248 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC93097F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:53:33,268 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:19,165 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930BEC0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:19,187 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:19,194 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9308140>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:19,214 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:19,218 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930BEC0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:19,232 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:19,237 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B530>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:19,259 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:30,047 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A8A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:30,067 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 16:54:30,069 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B530>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 16:54:30,088 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:00:00,090 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A000>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:00:00,111 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:00:00,840 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309F10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:00:00,859 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:15,918 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B530>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:15,940 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:15,946 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309F10>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:15,962 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:20,930 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A3F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:20,950 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:20,952 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B830>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:20,976 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:30,851 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9308410>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:30,871 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:03:30,875 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930B110>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:03:30,896 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:04:18,693 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC9309C40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:04:18,713 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:04:18,718 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930AD50>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:04:18,738 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-28 18:04:45,336 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EAC930A3F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-28 18:04:45,360 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 07:52:32,336 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001FD73888FB0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:52:32,340 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 07:52:32,341 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 07:52:35,398 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.51秒执行下一批
2026-01-29 07:52:37,905 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 07:52:43,793 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 07:52:43,793 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 07:52:43,794 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 07:52:43,794 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_zhihu_data执行失败：Expecting value: line 1 column 1 (char 0)
NoneType: None
2026-01-29 07:52:43,794 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-29 07:53:20,189 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001FD739E0E00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:53:20,207 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 07:54:51,407 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001FD739E37D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:54:51,424 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 07:55:02,168 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001FD739E3740>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:55:02,186 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 07:55:28,819 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001FD739E3500>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:55:28,834 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 07:55:28,835 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 07:55:32,450 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 07:55:32,450 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 07:55:32,450 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 07:55:32,452 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 07:55:32,452 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 07:55:32,455 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.72秒执行下一批
2026-01-29 07:55:35,171 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 07:55:39,596 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 07:55:39,598 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 07:55:39,598 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 07:55:39,598 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_zhihu_data执行失败：Expecting value: line 1 column 1 (char 0)
NoneType: None
2026-01-29 07:55:39,598 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-29 07:56:37,294 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000002B8968D8E30>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 07:56:37,306 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 07:56:37,306 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 07:56:39,706 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.53秒执行下一批
2026-01-29 07:56:42,246 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 07:56:45,742 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 07:56:45,742 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 07:56:45,742 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 07:56:45,742 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_zhihu_data执行失败：Expecting value: line 1 column 1 (char 0)
NoneType: None
2026-01-29 07:56:45,742 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合8个站点的热点数据
2026-01-29 08:00:01,869 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000257625E9B80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:00:01,889 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 08:32:48,393 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EE08658C20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:32:48,414 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 08:32:48,419 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EE0865BC20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:32:48,437 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 08:32:56,625 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001EE08658A40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:32:56,637 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 08:32:56,637 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 08:33:00,095 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.15秒执行下一批
2026-01-29 08:33:02,260 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 08:33:06,064 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 08:33:06,064 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 08:33:06,064 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 08:33:06,064 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 08:33:06,064 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-29 08:38:40,006 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A0D4B98C80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:38:40,018 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 08:38:40,018 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 08:38:43,195 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.78秒执行下一批
2026-01-29 08:38:45,978 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 08:38:50,993 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 08:38:50,993 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 08:38:50,993 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 08:38:50,993 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 08:38:50,993 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-29 08:39:23,034 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000001A0D4BC00B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 08:39:23,045 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 08:39:23,046 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 08:39:25,179 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.34秒执行下一批
2026-01-29 08:39:27,512 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 08:39:31,762 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 08:39:31,762 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 08:39:31,762 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 08:39:31,762 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 08:39:31,762 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-29 09:41:41,814 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000002520DFA8F80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 09:41:41,826 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共9个，每批并发5个
2026-01-29 09:41:41,827 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 09:41:44,713 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.07秒执行下一批
2026-01-29 09:41:46,792 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共4个
2026-01-29 09:41:51,192 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 09:41:51,192 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 09:41:51,192 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 09:41:51,192 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 09:41:51,192 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合9个站点的热点数据
2026-01-29 09:42:55,046 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000017A31E01E50>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 09:42:55,059 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共10个，每批并发5个
2026-01-29 09:42:55,060 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 09:42:57,343 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.45秒执行下一批
2026-01-29 09:42:59,808 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 09:43:02,829 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 09:43:02,830 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 09:43:02,830 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 09:43:02,830 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 09:43:02,830 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点小红书，共10条热点数据
2026-01-29 09:43:02,830 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合10个站点的热点数据
2026-01-29 09:46:12,103 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x000002BC3B251D90>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 09:46:12,116 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共10个，每批并发5个
2026-01-29 09:46:12,117 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 09:46:14,394 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.96秒执行下一批
2026-01-29 09:46:17,360 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点小红书，共10条热点数据
2026-01-29 09:46:20,578 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合10个站点的热点数据
2026-01-29 10:00:33,022 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000225B5AE1B80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 10:00:33,036 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 10:00:33,047 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x00000225B5AE3AD0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 10:00:33,062 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 10:01:02,139 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000023146C459D0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 10:01:02,150 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共10个，每批并发5个
2026-01-29 10:01:02,150 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 10:01:04,683 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.88秒执行下一批
2026-01-29 10:01:07,577 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点豆瓣电影，共10条热点数据
2026-01-29 10:01:10,462 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合10个站点的热点数据
2026-01-29 11:34:22,030 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117F9C70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 11:34:22,039 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共11个，每批并发5个
2026-01-29 11:34:22,040 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 11:34:24,917 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.87秒执行下一批
2026-01-29 11:34:27,792 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点豆瓣电影，共10条热点数据
2026-01-29 11:34:31,227 - MyApp - INFO - crawler_executor.py:80 - 第2批爬虫执行完成，延迟2.77秒执行下一批
2026-01-29 11:34:34,008 - MyApp - INFO - crawler_executor.py:49 - 执行第3批爬虫，共1个
2026-01-29 11:34:37,451 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点懂车帝，共10条热点数据
2026-01-29 11:34:37,451 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合11个站点的热点数据
2026-01-29 11:34:55,644 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11865700>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 11:34:55,664 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 11:52:19,244 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11866990>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 11:52:19,274 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 11:52:19,282 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118662A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 11:52:19,303 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:00:00,083 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11865940>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:00:00,099 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:29:57,831 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118646B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:29:57,843 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共11个，每批并发5个
2026-01-29 12:29:57,843 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 12:30:01,148 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.19秒执行下一批
2026-01-29 12:30:03,329 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 12:30:33,348 - MyApp - ERROR - crawler_executor.py:61 - 爬虫get_csdn_data执行失败：tuple indices must be integers or slices, not str
NoneType: None
2026-01-29 12:30:33,348 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 12:30:33,348 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 12:30:33,349 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 12:30:33,349 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点豆瓣电影，共10条热点数据
2026-01-29 12:30:33,349 - MyApp - INFO - crawler_executor.py:80 - 第2批爬虫执行完成，延迟2.47秒执行下一批
2026-01-29 12:30:35,812 - MyApp - INFO - crawler_executor.py:49 - 执行第3批爬虫，共1个
2026-01-29 12:30:38,662 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点懂车帝，共10条热点数据
2026-01-29 12:30:38,662 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合10个站点的热点数据
2026-01-29 12:30:51,514 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117F9AF0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:30:51,536 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:30:51,536 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117F9490>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:30:51,565 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:30:58,052 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117F9DC0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:30:58,071 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:33:35,096 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117FB980>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:33:35,115 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:33:35,119 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117F9850>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:33:35,141 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 12:33:42,966 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117FA480>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 12:33:42,976 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共11个，每批并发5个
2026-01-29 12:33:42,976 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 12:33:45,268 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.01秒执行下一批
2026-01-29 12:33:47,283 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点豆瓣电影，共10条热点数据
2026-01-29 12:33:51,057 - MyApp - INFO - crawler_executor.py:80 - 第2批爬虫执行完成，延迟2.51秒执行下一批
2026-01-29 12:33:53,563 - MyApp - INFO - crawler_executor.py:49 - 执行第3批爬虫，共1个
2026-01-29 12:33:56,281 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点懂车帝，共10条热点数据
2026-01-29 12:33:56,281 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合11个站点的热点数据
2026-01-29 15:32:40,306 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823230>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:32:40,325 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:32:40,336 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A117FB140>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:32:40,359 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:44:21,593 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118202C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:44:21,619 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:44:21,625 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118222A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:44:21,645 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:55:18,379 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11820F20>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:55:18,401 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:55:18,404 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823620>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:55:18,422 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:55:30,184 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11820A40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:55:30,206 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:55:30,212 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822180>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:55:30,234 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:58:32,342 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118221E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:58:32,354 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:58:47,299 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822D80>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:58:47,320 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 15:58:56,531 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11821A00>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 15:58:56,551 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:03:59,847 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822B40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:03:59,861 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:04:38,279 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822E70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:04:38,300 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:05:45,766 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823080>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:05:45,788 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:09:17,867 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11821AF0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:09:17,888 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:09:18,774 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823D70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:09:18,794 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:15,082 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118214F0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:15,104 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:16,766 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11820560>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:16,786 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:17,166 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11820B30>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:17,188 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:18,935 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823470>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:18,958 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:18,960 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823C50>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:18,978 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:54,028 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822B40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:54,049 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:12:54,055 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822A50>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:12:54,072 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:13:19,671 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822270>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:13:19,691 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:13:19,698 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11823710>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:13:19,719 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:13:21,186 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822E70>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:13:21,206 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:16:41,383 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118218E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:16:41,399 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:16:41,411 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11821220>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:16:41,432 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:23:25,832 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11821BE0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:23:25,864 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:23:25,867 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118221E0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:23:25,882 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:23:46,385 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118207A0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:23:46,406 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:23:46,406 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118217C0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:23:46,433 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:38:27,100 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822B40>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:38:27,117 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:38:27,124 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A118230B0>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:38:27,147 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:45:01,863 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11821220>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:45:01,886 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:45:01,891 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822240>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:45:01,909 - MyApp - INFO - crawler_executor.py:34 - 从redis中获取数据
2026-01-29 16:47:19,539 - MyApp - INFO - redis.py:14 - redis_client:<redis.client.Redis(<redis.connection.ConnectionPool(<redis.connection.Connection(db=0,username=None,password=None,socket_timeout=None,encoding=utf-8,encoding_errors=strict,decode_responses=True,retry_on_error=[],retry=<redis.retry.Retry object at 0x0000028A11822300>,health_check_interval=0,client_name=None,lib_name=redis-py,lib_version=6.4.0,redis_connect_func=None,credential_provider=None,protocol=2,host=10.199.197.150,port=6380,socket_connect_timeout=None,socket_keepalive=None,socket_keepalive_options=None)>)>)>
2026-01-29 16:47:19,550 - MyApp - INFO - crawler_executor.py:44 - 开始执行所有爬虫，共11个，每批并发5个
2026-01-29 16:47:19,550 - MyApp - INFO - crawler_executor.py:49 - 执行第1批爬虫，共5个
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点澎湃新闻，共20条热点数据
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点今日头条，共50条热点数据
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点百度，共51条热点数据
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点掘金，共50条热点数据
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点白鲸出海，共10条热点数据
2026-01-29 16:47:23,151 - MyApp - INFO - crawler_executor.py:80 - 第1批爬虫执行完成，延迟2.11秒执行下一批
2026-01-29 16:47:25,263 - MyApp - INFO - crawler_executor.py:49 - 执行第2批爬虫，共5个
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点csdn，共25条热点数据
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点acfun，共30条热点数据
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点第一财经，共10条热点数据
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点知乎，共30条热点数据
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点豆瓣电影，共10条热点数据
2026-01-29 16:47:28,665 - MyApp - INFO - crawler_executor.py:80 - 第2批爬虫执行完成，延迟2.18秒执行下一批
2026-01-29 16:47:30,864 - MyApp - INFO - crawler_executor.py:49 - 执行第3批爬虫，共1个
2026-01-29 16:47:32,398 - MyApp - INFO - crawler_executor.py:75 - 成功爬取站点懂车帝，共10条热点数据
2026-01-29 16:47:32,398 - MyApp - INFO - crawler_executor.py:83 - 所有爬虫执行完成，最终聚合11个站点的热点数据
